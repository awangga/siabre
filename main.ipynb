{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu\"  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from numpy import random\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "from dgmm import loadtrainandlabel,loadtestandlabel\n",
    "from lib.bdtb import simpanMSE, simpanMSEMiyawaki, plotDGMM,ubahkelistofchunks,simpanScore\n",
    "\n",
    "import time\n",
    "\n",
    "matlist=[]\n",
    "matlist.append('data/de_s1_V1_Ecc1to11_baseByRestPre_smlr_s1071119ROI_resol10_leave0_1x1_preprocessed.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matfile=matlist[0]\n",
    "train_data,label=loadtrainandlabel(matfile)\n",
    "testdt,testlb=loadtestandlabel(matfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s1_V1_Ecc1to11_baseByRestPre_smlr_s1071119ROI_resol10_figRecon_linComb-errFuncImageNonNegCon_1x1_maxProbLabel_dimNorm.mat\n"
     ]
    }
   ],
   "source": [
    "predm,labelm,msem=simpanMSEMiyawaki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=testlb.astype('float32')\n",
    "y=testdt.astype('float32')\n",
    "z=predm.astype('float32')\n",
    "\n",
    "#split data train dan testing untuk keperluan random \n",
    "X_train, X_test, Y_train, Y_test, Miyawaki_1, Miyawaki_2 = train_test_split( x, y, z,test_size=20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 10\n",
    "#menambahkan channel di bagian belakang, jika tidak dipakai hapus saja angka 1 dibelakangnya\n",
    "X_train = X_train.reshape([X_train.shape[0], resolution, resolution])\n",
    "X_test = X_test.reshape([X_test.shape[0], resolution, resolution])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape : \n",
      "(100, 10, 10)\n",
      "Y_train.shape\n",
      "(100, 967)\n",
      "X_test.shape\n",
      "(20, 10, 10)\n",
      "Y_test.shape\n",
      "(20, 967)\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))   \n",
    "Y_train = min_max_scaler.fit_transform(Y_train)     \n",
    "Y_test = min_max_scaler.transform(Y_test)\n",
    "\n",
    "print ('X_train.shape : ')\n",
    "print (X_train.shape)\n",
    "print ('Y_train.shape')\n",
    "print (Y_train.shape)\n",
    "print ('X_test.shape')\n",
    "print (X_test.shape)\n",
    "print ('Y_test.shape')\n",
    "print (Y_test.shape)\n",
    "numTrn=X_train.shape[0]\n",
    "numTest=X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=1\n",
    "for stim in X_train:\n",
    "  plt.imsave('./stim/'+str(filename)+'.png',np.rot90(np.fliplr(stim)))\n",
    "  filename=filename+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "membuat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)\n",
    "\n",
    "def initialize_bias(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(featurelength):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(featurelength)\n",
    "    right_input = Input(featurelength)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_dim=featurelength, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(100,activation='relu',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 967)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 967)]        0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 100)          5706340     ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 100)          0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 100)          10100       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,716,440\n",
      "Trainable params: 5,716,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(967)\n",
    "model.summary()\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "batch_size = 4 #32 # jumlah data training\n",
    "n_iter = 20000 # No. of training iterations\n",
    "N_way = 2 #20 # how many classes for testing one-shot tasks, jumlah data testing \n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './weights/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berikutnya adalah kelompokkan input sinyal fmri dengan target yang mirip, karena inputan 2 fmri dengan outputan 1 label stimulus yang sama. kita kelompokkan dulu ya ges. bikinan sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,s=\"train\"):\n",
    "      e_list=[11,14,18,20,30,37,46,51,60,65,67,78]#70\n",
    "      Y_train_e = []\n",
    "      X_train_e = []\n",
    "      for e in e_list :\n",
    "        Y_train_e.append(Y_train[e-1])\n",
    "        X_train_e.append(X_train[e-1])\n",
    "\n",
    "      Y_train_e = np.array(Y_train_e)\n",
    "      X_train_e = np.array(X_train_e)\n",
    "\n",
    "\n",
    "      Y_train_e1,Y_train_e2=np.array_split(Y_train_e, 2)\n",
    "      #X_train_e1,X_train_e2=np.array.split(X_train_e,2)\n",
    "      X_train_e[:5]\n",
    "      pairs = Y_train_e1[:batch_size],Y_train_e2[:batch_size]\n",
    "      targets = X_train_e[:batch_size]\n",
    "      return pairs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bikinan sendiri modif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\", language=None):\n",
    "    \"\"\"Create pairs of test image, support set for testing N way one-shot learning. bikin untuk genrate data testing\"\"\"\n",
    "    e_list=[11,14,18,20,30,37,46,51,60,65,67,78]#70\n",
    "    Y_train_e = []\n",
    "    X_train_e = []\n",
    "    for e in e_list :\n",
    "      Y_train_e.append(Y_train[e-1])\n",
    "      X_train_e.append(X_train[e-1])\n",
    "\n",
    "    Y_train_e = np.array(Y_train_e)\n",
    "    X_train_e = np.array(X_train_e)\n",
    "\n",
    "    Y_train_e1,Y_train_e2=np.array_split(Y_train_e, 2)\n",
    "    #X_train_e1,X_train_e2=np.array.split(X_train_e,2)\n",
    "    if s == 'train':\n",
    "        pairs = Y_train_e1[:batch_size],Y_train_e2[:batch_size]\n",
    "        targets = X_train_e[:batch_size]\n",
    "    else:\n",
    "        pairs = Y_train_e1[-N:],Y_train_e2[-N:]\n",
    "        targets = X_train_e[-N:]\n",
    "    return pairs, targets\n",
    "\n",
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2ccb58c476f33ba3e3aee7ac07234ef6b8217ef24ad64d2a7d4fed1a57c1cd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
